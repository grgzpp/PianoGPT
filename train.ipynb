{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nanogpt_model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"out\"\n",
    "eval_interval = 100\n",
    "log_interval = 1\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "init_from = \"resume\" # \"scratch\" or \"resume\"\n",
    "\n",
    "# data\n",
    "gradient_accumulation_steps = 4 # used to simulate larger batch sizes\n",
    "batch_size = 8 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 512\n",
    "vocab_size = 217\n",
    "\n",
    "# model\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning 0.1\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 #5e-5 for finetuning # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 #5e-6 for finetuning # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = \"bfloat16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"float16\" # \"float32\", \"bfloat16\", or \"float16\", the latter will auto implement a GradScaler\n",
    "compile_model = False # use PyTorch 2.0 to compile the model to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=vocab_size, dropout=dropout) # start with model_args from command line\n",
    "if init_from == \"scratch\":\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    iter_num = 0\n",
    "    best_val_loss = 1e9\n",
    "elif init_from == \"resume\":\n",
    "    print(f\"Resuming model from {out_dir}\")\n",
    "    ckpt_path = os.path.join(out_dir, \"ckpt_beethoven.pt\") # ckpt_pre_trained.pt\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "    for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint[\"model\"]\n",
    "\n",
    "    unwanted_prefix = \"_orig_mod.\"\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint[\"iter_num\"]\n",
    "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == \"resume\":\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "checkpoint = None\n",
    "\n",
    "if compile_model:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_df = pd.read_csv(\"midi_dataset_512.csv\")\n",
    "midi_df_len = len(midi_df)\n",
    "print(f\"Total chunks: {midi_df_len}\")\n",
    "midi_df = midi_df.sample(frac=1).reset_index(drop=True) # Shuffle and drop indexes\n",
    "midi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_tokens_dataset = np.load(os.path.join(\"midi_tokens_dataset_512.npy\"))\n",
    "midi_tokens_dataset = torch.tensor(midi_tokens_dataset, dtype=torch.int64)\n",
    "print(\"midi_tokens_dataset shape\", midi_tokens_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = torch.from_numpy(np.load(\"unique_tokens.npy\"))\n",
    "vocab_size = len(unique_tokens)\n",
    "print(\"vocab_size\", vocab_size)\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_mapping = {unique_tokens[i].item(): i for i in range(len(unique_tokens))}\n",
    "tokens_unmapping = {i: unique_tokens[i].item() for i in range(len(unique_tokens))}\n",
    "print(tokens_mapping)\n",
    "print(tokens_unmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens(tokens, tokens_mapping):\n",
    "    mapped_tokens = torch.zeros_like(tokens)\n",
    "    for i in range(len(tokens)):\n",
    "        mapped_tokens[i] = tokens_mapping[tokens[i].item()]\n",
    "    return mapped_tokens\n",
    "\n",
    "def unmap_tokens(mapped_tokens, tokens_unmapping):\n",
    "    unmapped_tokens = torch.zeros_like(mapped_tokens)\n",
    "    for i in range(len(mapped_tokens)):\n",
    "        unmapped_tokens[i] = tokens_unmapping[mapped_tokens[i].item()]\n",
    "    return unmapped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_len = round(midi_df_len*0.99)\n",
    "print(f\"Training set len: {train_set_len}, Validation set len: {midi_df_len - train_set_len}.\")\n",
    "midi_df_train = midi_df.iloc[:train_set_len].reset_index(drop=True)\n",
    "midi_df_val = midi_df.iloc[train_set_len:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, midi_tokens_dataset, midi_df):\n",
    "        self.midi_tokens_dataset = midi_tokens_dataset[midi_df[\"chunk_idx\"].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.midi_tokens_dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_tokens = self.midi_tokens_dataset[idx]\n",
    "        chunk_tokens = map_tokens(chunk_tokens, tokens_mapping)\n",
    "        x = chunk_tokens[:-1]\n",
    "        y = chunk_tokens[1:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MidiDataset(midi_tokens_dataset, midi_df_train)\n",
    "val_dataset = MidiDataset(midi_tokens_dataset, midi_df_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_val_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(len(val_loader))\n",
    "    print(\"Calculating validation loss...\")\n",
    "    for i, (X, Y) in enumerate(tqdm(val_loader)):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        \"\"\" if i >= eval_iters:\n",
    "            break \"\"\"\n",
    "        with ctx:\n",
    "            _, loss = model(X, Y)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean().item()\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "    micro_step = 0\n",
    "    for X, Y in train_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        if iter_num % eval_interval == 0:\n",
    "            val_loss = estimate_val_loss()\n",
    "            print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n",
    "          \n",
    "            if val_loss < best_val_loss or always_save_checkpoint:\n",
    "                best_val_loss = val_loss\n",
    "                if local_iter_num > 0:\n",
    "                    checkpoint = {\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"model_args\": model_args,\n",
    "                        \"iter_num\": iter_num,\n",
    "                        \"best_val_loss\": best_val_loss\n",
    "                    }\n",
    "                    print(f\"saving checkpoint to {out_dir}\")\n",
    "                    torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
    "        if local_iter_num == 0 and eval_only:\n",
    "            break\n",
    "\n",
    "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "        # and using the GradScaler if data type is float16\n",
    "\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        micro_step += 1\n",
    "        if micro_step >= gradient_accumulation_steps:\n",
    "            micro_step = 0\n",
    "            # clip the gradient\n",
    "            if grad_clip != 0.0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            # step the optimizer and scaler if training in fp16\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # flush the gradients as soon as we can, no need for this memory anymore\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            t0 = t1\n",
    "            if iter_num % log_interval == 0:\n",
    "                # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "                lossf = loss.item() * gradient_accumulation_steps\n",
    "                print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "        \n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "    if iter_num > max_iters:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiocraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
